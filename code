# ==============================
# Forest Cover Type Classification
# ==============================

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_covtype
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# ------------------------------
# 1. Load dataset from sklearn
# ------------------------------
data = fetch_covtype(as_frame=True)
df = data.frame
df['Cover_Type'] = df['Cover_Type'].astype('category')

print("Dataset shape:", df.shape)
print(df.head())

# ------------------------------
# 2. Split features & target
# ------------------------------
X = df.drop('Cover_Type', axis=1)
y = df['Cover_Type']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# ------------------------------
# 3. Random Forest - Multi-class
# ------------------------------
rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("\nRandom Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, cmap='Blues')
plt.title("Random Forest - Multi-class")
plt.show()

# Feature importance
importances_rf = pd.Series(rf.feature_importances_, index=X.columns)
importances_rf.sort_values(ascending=False).head(10).plot(kind='bar', title="Top 10 Features (RF)")
plt.show()

# ------------------------------
# 4. XGBoost - Multi-class
# ------------------------------
xgb = XGBClassifier(
    objective='multi:softmax',
    num_class=len(y.cat.categories),
    eval_metric='mlogloss',
    n_estimators=300,
    learning_rate=0.1,
    max_depth=8,
    random_state=42,
    n_jobs=-1
)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

print("\nXGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

ConfusionMatrixDisplay.from_estimator(xgb, X_test, y_test, cmap='Blues')
plt.title("XGBoost - Multi-class")
plt.show()

# ------------------------------
# 5. Binary classification example (Imbalanced)
# ------------------------------
# Example: Class 1 (Spruce/Fir) vs all others
df_binary = df.copy()
df_binary['Cover_Type'] = (df_binary['Cover_Type'] == 1).astype(int)

X_bin = df_binary.drop('Cover_Type', axis=1)
y_bin = df_binary['Cover_Type']

Xb_train, Xb_test, yb_train, yb_test = train_test_split(
    X_bin, y_bin, test_size=0.2, stratify=y_bin, random_state=42
)

rf_bin = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1)
rf_bin.fit(Xb_train, yb_train)
yb_pred_rf = rf_bin.predict(Xb_test)

print("\nRandom Forest - Binary (Class 1 vs Others):")
print(classification_report(yb_test, yb_pred_rf))

ConfusionMatrixDisplay.from_estimator(rf_bin, Xb_test, yb_test, cmap='Blues')
plt.title("Random Forest - Binary")
plt.show()
